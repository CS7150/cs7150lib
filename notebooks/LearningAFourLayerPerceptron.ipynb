{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you are on Google Colab, this sets up everything needed.\n",
    "# If not, you will want to pip install the cs7150lib as shown below.\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "pip install git+https://github.com/cs7150/cs7150lib@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd2fc0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CS7150/cs7150lib/blob/main/notebooks/LearningAFourLayerPerceptron.ipynb)\n",
    "\n",
    "# Learning a four-layer perceptron\n",
    "\n",
    "We learned in Lecture 1 that Rosenblatt's Perceptron was limited because it only learned one layer.  It was a *shallow* learning architecture.\n",
    "\n",
    "We want to learn *deep* layers.\n",
    "\n",
    "To do it, we start with the simple idea from [Rumelhart 1986](https://papers.baulab.info/Rumelhart-1986.pdf): use *derivatives* to figure out the updates.\n",
    "\n",
    "This works surprisingly well, but it can fail if the derivatives get too big or too small, for example, if the network has too many layers.  We will explore these situations in this notebook.\n",
    "\n",
    "This notebook is designed to be able to work on Google colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe811fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh\n",
    "from matplotlib import pyplot as plt\n",
    "from cs7150 import seeded_net, non_linearly_separable_data, Sign\n",
    "from cs7150 import MLPHistoryWidget, LossSurfaceWidget\n",
    "from baukit import show, pbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7deca0",
   "metadata": {},
   "source": [
    "# Adding more layers to Rosenblatt's setup\n",
    "\n",
    "The original perceptron would have had one trainable neuron with an input weight for the horizontal position of a dot, and a second input weight for the vertical position of the dot.  In other words, it would have had a two-dimenaional input.\n",
    "\n",
    "We will make the setup more powerful by adding three more trainable layers before this last neuron.  Each layer will take two inputs and produce two outputs, as illustrated here:\n",
    "\n",
    "<img src=\"https://cs7150.baulab.info/2022-Fall/colab/mlp-architecture.png\">\n",
    "\n",
    "The code is below.\n",
    "\n",
    "## Exercise 1.  Run the multilayer perceptron.\n",
    "\n",
    "Run the multilayer perceptron below, without changing any code.\n",
    "\n",
    "Look at the training curve after 1000 steps.  What (doesn't) happen?\n",
    "\n",
    "That is because the derivatives are always zero.  You will fix this in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Exercise 1, just read and run the code.\n",
    "data, labels = non_linearly_separable_data()\n",
    "\n",
    "mlp = torch.nn.Sequential(OrderedDict([\n",
    "    ('layer1', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer2', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer3', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer4', Sequential(Linear(2, 1), Sign()))\n",
    "]))\n",
    "seeded_net(1, mlp) # Seed the parameters pseudorandomly\n",
    "\n",
    "history = []\n",
    "learning_rate = 0.01\n",
    "for iteration in pbar(range(1000)):\n",
    "    preds = mlp(data)[:,0]\n",
    "    loss = ((preds - labels) ** 2).mean()\n",
    "    grads = torch.autograd.grad(loss, mlp.parameters())\n",
    "    with torch.no_grad():\n",
    "        accuracy = (preds.sign() == labels).sum().item() / len(labels)\n",
    "        # Remember a history of network weights, losses, and accuracies\n",
    "        history.append((deepcopy(mlp), dict(loss=loss.item(), accuracy=accuracy)))\n",
    "        # Update the network parameters\n",
    "        for (name, parameter), grad in zip(mlp.named_parameters(), grads):\n",
    "            parameter -= learning_rate * grad\n",
    "# Visualize the history.\n",
    "MLPHistoryWidget(data=data, labels=labels, history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ec9bb",
   "metadata": {},
   "source": [
    "## Exercise 2.  Make the network differentiable using Tanh().\n",
    "\n",
    "Copy the Exercise 1 code into the cell below, and then edit it so that instead of using the Sign() step nonlinearity, the network uses the Tanh() sigmoid nonlinearity.  Try things again.  What's the difference?\n",
    "\n",
    "What accuracy is achieved in 1000 training iterations?\n",
    "\n",
    "Do not train the network to a global optimum yet, because we will want to visualize a splot on a non-optimized loss surface in Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d5272",
   "metadata": {},
   "source": [
    "## Exercise 3. Visualize the loss surface.\n",
    "\n",
    "The following code prints every individual scalar parameter in the model.  There are 21 parameters.\n",
    "\n",
    "Your task is to visualize the loss surface by plotting the 21 loss curves when each individual parameter is offset by -10.0 to +10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, param in mlp.named_parameters():\n",
    "        for index in numpy.ndindex(param.shape):\n",
    "            original_w = param[index].detach().item()\n",
    "            print(f'Value of {name}{index}', original_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e1991",
   "metadata": {},
   "source": [
    "The following code shows the loss curve varying just two of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (0, 0)\n",
    "offsets = torch.linspace(-10.0, 10.0, 101)\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for param in [mlp.layer1[0].weight, mlp.layer2[0].weight]:\n",
    "        losses = []\n",
    "        original_w = param[index].detach().item()\n",
    "        for offset in offsets:\n",
    "            param[index] = original_w + offset\n",
    "            preds = mlp(data)[:,0]\n",
    "            loss = ((preds - labels) ** 2).mean()\n",
    "            losses.append(loss.item())\n",
    "        param[index] = original_w\n",
    "        fig, ax = plt.subplots(figsize=(3,2))\n",
    "        ax.set_title(f'Loss')\n",
    "        ax.plot(offsets, losses)\n",
    "        results.append(fig)\n",
    "        plt.close()\n",
    "show([results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c6108",
   "metadata": {},
   "source": [
    "Now, with the two examples above in mind, make a function called `show_loss_surface` that takes `mlp` as an argument, and then shows loss curves for all 21 parameters, labeling each graph with the name of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a67f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2936abd9",
   "metadata": {},
   "source": [
    "You can also use `LossSurfaceWidget(mlp)` to visualize the interaction between two parameters at once.  Use this to visualize some pairs of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863b63b",
   "metadata": {},
   "source": [
    "## Exercise 4. Adjust the learning rate to find a global optimum.\n",
    "\n",
    "Copy the Exercise 2 code again, and adjust the learning rate to speeds up training.\n",
    "\n",
    "Try to find a solution that looks like a global optimum, and now plot the loss curves using `show_loss_surface` again.  Does the result look more like a global optimium?\n",
    "\n",
    "Now change the seed, and look for a case that gets stuck before reaching an optimum, and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 4 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed787f",
   "metadata": {},
   "source": [
    "## Exercise 5.  Monitor the gradients\n",
    "\n",
    "Copy the Exercise 4 code below, and use any seed which is getting stuck for you.\n",
    "\n",
    "Use code like the following to create a add a list of gradients to the training history. \n",
    "\n",
    "```\n",
    "        gradlog = {name + ' grad': grad.norm().item()\n",
    "                   for (name, _), grad in zip(mlp.named_parameters(), grads) if 'weight' in name}\n",
    "        history.append((deepcopy(mlp), dict(loss=loss.item(), accuracy=accuracy, **gradlog)))\n",
    "```\n",
    "\n",
    "Do you notice any trends with the gradients during training?   What happens when your training gets stuck?\n",
    "\n",
    "Sometimes, some layers have very different gradients than other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 5 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ed2f1",
   "metadata": {},
   "source": [
    "## Exercise 6. Experiment with very deep networks and weight initialization.\n",
    "\n",
    "[Kaiming He (2015)](https://papers.baulab.info/He-2015.pdf) and [Xavier Glorot (2010)](https://papers.baulab.info/Glorot-2010.pdf) wrote famous papers where they noticed that the scale of the weight initialization was often critical to successful training when the network is deeper than a few layers.  The `seeded_net` function initializes parameters with unit variance, but you can change this.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "   1. Make the network 6 layers deep or deeper by copying the early layers and giving them new names.\n",
    "   2. Monitor gradients as in exercise 4 above\n",
    "   3. Experiment with larger and smaller weights by adding code like this after `seeded_net`:\n",
    "\n",
    "```\n",
    "seeded_net(1, mlp) # Seed the parameters pseudorandomly\n",
    "with torch.no_grad():\n",
    "    for parameter in mlp.parameters():\n",
    "        parameter *= 0.7\n",
    "```\n",
    "\n",
    "What is the impact of initialization on initial gradients at iteration 0?\n",
    "\n",
    "Typically you either have **vanishing gradients** or **exploding gradients** unless initialization is just right.  Which are you seeing with this architecture?\n",
    "\n",
    "The famous [AlexNet network, Krizhevsky (2012)](https://papers.baulab.info/Krizhevsky-2012.pdf) was 8 layers deep.  Can you get an 8-layer-deep version of this network to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f60779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 6 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a19031",
   "metadata": {},
   "source": [
    "## Exercise 7.  Experiment with wide and shallow networks.\n",
    "\n",
    "[George Cybenko (1989)](https://papers.baulab.info/Cybenko-1989.pdf) and\n",
    "[Kurt Hornik (1991)](https://papers.baulab.info/also/Hornik-1991.pdf)\n",
    "wrote famous papers that showed that any continuous function can be approximated using\n",
    "just a couple layers, if you use enough neurons.\n",
    "\n",
    "Try training a network with just two layers. How many neurons do you need to get good accuracy?\n",
    "\n",
    "[Dauphin (2014)](https://papers.baulab.info/also/Dauphin-2014.pdf) has argued that, in practice, when you have a high dimensional network, you do not run into local minima as often (he argues that the main problem is flat saddle points instead).\n",
    "\n",
    "Do you see any other phenomenon in the shape of the learned function of a wide shallow network that bothers you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 7 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a36324",
   "metadata": {},
   "source": [
    "## Exercise 8.  Experiment with networks that are both wide and shallow.\n",
    "\n",
    "Classical machine learning theory would suggest that the problems you saw in Exercise 6 with very shallow networks with many neurons would get even worse if you added even more neurons on more layers.\n",
    "\n",
    "However, classical theory does not explain the success of deep networks in this setting.  See [Nakkiran 2019](https://papers.baulab.info/also/Nakkiran-2019.pdf) for a discussion of some of the issues.\n",
    "\n",
    "Try it out.  Create a network with 8 layers, with 10 hidden dimensions at every layer (except the input of the network should remain 2 dimensional and the output should be one dimensional).\n",
    "\n",
    "Find a weight initialization and learning rate that seems to be stable.\n",
    "\n",
    "* Does the network converge to an accurate solution for different weights?\n",
    "* How does the learned function compare to the functions learned in Exercise 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a84c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 8 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
