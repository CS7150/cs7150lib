{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# If you are on Google Colab, this sets up everything needed.\n",
    "# If not, you will want to pip install the cs7150lib as shown below.\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "pip install git+https://github.com/cs7150/cs7150lib@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd2fc0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CS7150/cs7150lib/blob/main/notebooks/LearningAFourLayerPerceptron.ipynb)\n",
    "\n",
    "# Learning a four-layer perceptron\n",
    "\n",
    "We learned in Lecture 1 that Rosenblatt's Perceptron was limited because it only learned one layer.  It was a *shallow* learning architecture.\n",
    "\n",
    "We want to learn *deep* layers.\n",
    "\n",
    "To do it, we start with the simple idea from [Rumelhart 1986](https://papers.baulab.info/Rumelhart-1986.pdf): use *derivatives* to figure out the updates.\n",
    "\n",
    "But, to make this work, there are a lot of details that need to be done right.  Fortunately our CPUs run a lot faster than Rosenblatt's and Rumelharts.  We can do experiments quickly, and we will explore the important details in this notebook.  You do not need a GPU for this exercise.\n",
    "\n",
    "This notebook is designed to be able to work on Google colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe811fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh\n",
    "from matplotlib import pyplot as plt\n",
    "from cs7150 import seeded_net, non_linearly_separable_data, MLPHistoryWidget, Sign\n",
    "from baukit import show, pbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7deca0",
   "metadata": {},
   "source": [
    "# Adding more layers to Rosenblatt's setup\n",
    "\n",
    "The original perceptron would have had one trainable neuron with an input weight for the horizontal position of a dot, and a second input weight for the vertical position of the dot.  In other words, it would have had a two-dimenaional input.\n",
    "\n",
    "We will make the setup more powerful by adding three more trainable layers before this last neuron.  Each layer will take two inputs and produce two outputs, as illustrated here:\n",
    "\n",
    "<img src=\"https://cs7150.baulab.info/2022-Fall/colab/mlp-architecture.png\">\n",
    "\n",
    "The code is below.\n",
    "\n",
    "## Exercise 1.  Run the multilayer perceptron.\n",
    "\n",
    "Run the multilayer perceptron below, without changing any code.\n",
    "\n",
    "Look at the training curve after 1000 steps.  What (doesn't) happen?\n",
    "\n",
    "That is because the derivatives are always zero.  You will fix this in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Exercise 1, just read and run the code.\n",
    "data, labels = non_linearly_separable_data()\n",
    "\n",
    "mlp = torch.nn.Sequential(OrderedDict([\n",
    "    ('layer1', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer2', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer3', Sequential(Linear(2, 2), Sign())),\n",
    "    ('layer4', Sequential(Linear(2, 1), Sign()))\n",
    "]))\n",
    "seeded_net(1, mlp) # Seed the parameters pseudorandomly\n",
    "\n",
    "history = []\n",
    "learning_rate = 0.01\n",
    "for iteration in pbar(range(1000)):\n",
    "    preds = mlp(data)[:,0]\n",
    "    loss = ((preds - labels) ** 2).mean()\n",
    "    grads = torch.autograd.grad(loss, mlp.parameters())\n",
    "    with torch.no_grad():\n",
    "        accuracy = (preds.sign() == labels).sum().item() / len(labels)\n",
    "        # Remember a history of network weights, losses, and accuracies\n",
    "        history.append((deepcopy(mlp), dict(loss=loss.item(), accuracy=accuracy)))\n",
    "        # Update the network parameters\n",
    "        for (name, parameter), grad in zip(mlp.named_parameters(), grads):\n",
    "            parameter -= learning_rate * grad\n",
    "# Visualize the history.\n",
    "MLPHistoryWidget(data=data, labels=labels, history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ec9bb",
   "metadata": {},
   "source": [
    "## Exercise 2.  Make the network differentiable using Tanh().\n",
    "\n",
    "Copy the Exercise 1 code into the cell below, and then edit it so that instead of using the Sign() step nonlinearity, the network uses the Tanh() sigmoid nonlinearity.  Try things again.  What's the difference?\n",
    "\n",
    "What accuracy is achieved in 1000 training iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863b63b",
   "metadata": {},
   "source": [
    "## Exercise 3. Speed up training with a faster learning rate.\n",
    "\n",
    "Find a faster learning rate that speeds up training.\n",
    "\n",
    "Your goal is to achieve 99% accuracy in 300 training iterations.  Is it possible?\n",
    "\n",
    "How does your training procedure work if you change the seed, e.g., `seeded_net(2, mlp)` instead of `seeded_net(1, mlp)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed787f",
   "metadata": {},
   "source": [
    "## Exercise 4.  Monitor the gradients\n",
    "\n",
    "Copy the code again below, and use the seed of 2, or any other seed which is getting stuck for you.\n",
    "\n",
    "Use code like the following to create a add a list of gradients to the training history. \n",
    "\n",
    "```\n",
    "        gradlog = {name + ' grad': grad.norm().item()\n",
    "                   for (name, _), grad in zip(mlp.named_parameters(), grads) if 'weight' in name}\n",
    "        history.append((deepcopy(mlp), dict(loss=loss.item(), accuracy=accuracy, **gradlog)))\n",
    "```\n",
    "\n",
    "Do you notice any trends with the gradients during training?   What happens when your training gets stuck?\n",
    "\n",
    "Sometimes, some layers have very different gradients than other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 4 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ed2f1",
   "metadata": {},
   "source": [
    "## Exercise 5. Experiment with very deep networks and weight initialization.\n",
    "\n",
    "[Kaiming He (2015)](https://papers.baulab.info/He-2015.pdf) and [Xavier Glorot (2010)](https://papers.baulab.info/Glorot-2010.pdf) wrote famous papers where they noticed that the scale of the weight initialization was often critical to successful training when the network is deeper than a few layers.  The `seeded_net` function initializes parameters with unit variance, but you can change this.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "   1. Make the network 6 layers deep or deeper by copying the early layers and giving them new names.\n",
    "   2. Monitor gradients as in exercise 4 above\n",
    "   3. Experiment with larger and smaller weights by adding code like this after `seeded_net`:\n",
    "\n",
    "```\n",
    "seeded_net(1, mlp) # Seed the parameters pseudorandomly\n",
    "with torch.no_grad():\n",
    "    for parameter in mlp.parameters():\n",
    "        parameter *= 0.7\n",
    "```\n",
    "\n",
    "What is the impact of initialization on initial gradients at iteration 0?\n",
    "\n",
    "Typically you either have **vanishing gradients** or **exploding gradients** unless initialization is just right.  Which are you seeing with this architecture?\n",
    "\n",
    "The famous [AlexNet network, Krizhevsky (2012)](https://papers.baulab.info/Krizhevsky-2012.pdf) was 8 layers deep.  Can you get an 8-layer-deep version of this network to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f60779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 5 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a19031",
   "metadata": {},
   "source": [
    "## Exercise 6.  Experiment with wide and shallow networks.\n",
    "\n",
    "[George Cybenko (1989)](https://papers.baulab.info/Cybenko-1989.pdf) and\n",
    "[Kurt Hornik (1991)](https://papers.baulab.info/also/Hornik-1991.pdf)\n",
    "wrote famous papers that showed that any continuous function can be approximated using\n",
    "just a couple layers, if you use enough neurons.\n",
    "\n",
    "Try training a network with just two layers. How many neurons do you need to get good accuracy?\n",
    "\n",
    "[Dauphin (2014)](https://papers.baulab.info/also/Dauphin-2014.pdf) has argued that, in practice, when you have a high dimensional network, you do not run into local minima as often (he argues that the main problem is flat saddle points instead).\n",
    "\n",
    "Do you see any other phenomenon in the shape of the learned function of a wide shallow network that bothers you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 6 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a36324",
   "metadata": {},
   "source": [
    "## Exercise 7.  Experiment with networks that are both wide and shallow.\n",
    "\n",
    "Classical machine learning theory would suggest that the problems you saw in Exercise 6 with very shallow networks with many neurons would get even worse if you added even more neurons on more layers.\n",
    "\n",
    "However, classical theory does not explain the success of deep networks in this setting.  See [Nakkiran 2019](https://papers.baulab.info/also/Nakkiran-2019.pdf) for a discussion of some of the issues.\n",
    "\n",
    "Try it out.  Create a network with 8 layers, with 10 hidden dimensions at every layer (except the input of the network should remain 2 dimensional and the output should be one dimensional).\n",
    "\n",
    "Find a weight initialization and learning rate that seems to be stable.\n",
    "\n",
    "* Does the network converge to an accurate solution for different weights?\n",
    "* How does the learned function compare to the functions learned in Exercise 6?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a84c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 7 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
