{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Vectors\n",
    "\n",
    "(This notebook is derived from the terrific notebook by Lisa Zhang; I have added normalization and cosine similarity to improve some of the analogy results.)\n",
    "\n",
    "The idea of learning an alternative representation/features/*embeddings* of data\n",
    "is prevalent in machine learning. You have seen how convolutional networks will\n",
    "learn generalized feature detectors. Good representations will\n",
    "make downstream tasks (like generating new data, clustering, computing distances)\n",
    "perform much better.\n",
    "\n",
    "GloVe embeddings provides a similar kind of pre-trained embeddings, but for **words**.\n",
    "\n",
    "You can think of the use of **GloVe embeddings** similarly the way you might use pre-trained\n",
    "network weights.  More information about GloVe is available here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "## GloVe Embeddings\n",
    "\n",
    "PyTorch makes it easy for us to use pre-trained GloVe embeddings.\n",
    "There are several variations of GloVe embeddings available; they differ in the corpus (data)\n",
    "that the embeddings are trained on, and the size (length) of each word embedding vector.\n",
    "\n",
    "These embeddings were trained by the authors of GloVe (Pennington et al. 2014),\n",
    "and are also available on the website https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "To load pre-trained GloVe embeddings, we'll use a package called `torchtext`.\n",
    "The package `torchtext` contains other useful tools for working with text\n",
    "that we will see later in the course. The documentation for torchtext\n",
    "GloVe vectors are available at: https://torchtext.readthedocs.io/en/latest/vocab.html#glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "# The first time you run this will download a ~823MB file\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=50)    # embedding size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what the embedding of the word \"car\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4769, -0.0846,  1.4641,  0.0470,  0.1469,  0.5082, -1.2228, -0.2261,\n",
       "         0.1931, -0.2976,  0.2060, -0.7128, -1.6288,  0.1710,  0.7480, -0.0619,\n",
       "        -0.6577,  1.3786, -0.6804, -1.7551,  0.5832,  0.2516, -1.2114,  0.8134,\n",
       "         0.0948, -1.6819, -0.6450,  0.6322,  1.1211,  0.1611,  2.5379,  0.2485,\n",
       "        -0.2682,  0.3282,  1.2916,  0.2355,  0.6147, -0.1344, -0.1324,  0.2740,\n",
       "        -0.1182,  0.1354,  0.0743, -0.6195,  0.4547, -0.3032, -0.2188, -0.5605,\n",
       "         1.1177, -0.3659])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a torch tensor with dimension `(50,)`. It is difficult to determine what each\n",
    "number in this embedding means, if anything. However, we know that there is structure\n",
    "in this embedding space. That is, distances in this embedding space is meaningful.\n",
    "\n",
    "## Measuring Distance\n",
    "\n",
    "To explore the structure of the embedding space, it is necessary to introduce\n",
    "a notion of *distance*. You are probably already familiar with the notion\n",
    "of the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
    "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
    "the Euclidean distance between $x$ and $y$:\n",
    "$\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "The PyTorch function `torch.norm` computes the 2-norm of a vector for us, so we \n",
    "can compute the Euclidean distance between two vectors like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8846)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "torch.norm(y - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative measure of distance is the **Cosine Similarity**.\n",
    "The cosine similarity measures the *angle* between two vectors,\n",
    "and has the property that it only considers the *direction* of the\n",
    "vectors, not their the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 1., 1.])[None]\n",
    "y = torch.tensor([2., 2., 2.])[None]\n",
    "torch.cosine_similarity(x, y) # should be one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is a *similarity* measure rather than a *distance* measure:\n",
    "The larger the similarity,\n",
    "the \"closer\" the word embeddings are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9218])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "torch.cosine_similarity(glove['cat'][None],\n",
    "                        glove['dog'][None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "Now that we have a notion of distance in our embedding space, we can talk\n",
    "about words that are \"close\" to each other in the embedding space.\n",
    "For now, let's use Euclidean distances to look at how close various words\n",
    "are to the word \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog 1.8846031427383423\n",
      "bike 5.048375129699707\n",
      "kitten 3.5068609714508057\n",
      "puppy 3.0644655227661133\n",
      "kite 4.210376262664795\n",
      "computer 6.030652046203613\n",
      "neuron 6.228669166564941\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "other = ['dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = torch.norm(glove[word] - glove[w]) # euclidean distance\n",
    "    print(w, float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can look through our entire vocabulary for words that are closest\n",
    "to a point in the embedding space -- for example, we can look for words\n",
    "that are closest to another word like \"cat\".\n",
    "\n",
    "Keep in mind that GloVe vectors are trained on **word co-occurrences**, and so\n",
    "words with similar embeddings will tend to co-occur with other words. For example,\n",
    "\"cat\" and \"dog\" tend to occur with similar other words---even more so than \"cat\"\n",
    "and \"kitten\" because these two words tend to occur in *different contexts*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 0.0\n",
      "dog 1.8846031\n",
      "rabbit 2.4572797\n",
      "monkey 2.8102052\n",
      "cats 2.8972251\n",
      "rat 2.9455352\n",
      "beast 2.9878407\n",
      "monster 3.0022194\n",
      "pet 3.0396757\n",
      "snake 3.0617998\n",
      "puppy 3.0644655\n"
     ]
    }
   ],
   "source": [
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[0:n+1]:                 # take the top n\n",
    "        print(glove.itos[idx], difference)\n",
    "\n",
    "print_closest_words(glove[\"cat\"], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: define a closest-cosine function\n",
    "\n",
    "Now define `print_closest_cosine` to be just like `print_closest_words`, but use the `torch.cosine_similarity` function.  ([Documentation here](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html).)\n",
    "\n",
    "Hints:\n",
    " 1. You will need to unsqueeze `vec` e.g., using `vec[None]`\n",
    " 2. You will need to use the reverse sort order since it's a similarity instead of a distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "def print_closest_cosine(vec, n=5):\n",
    "    print('Your implementation of print_closest_cosine needed')\n",
    "\n",
    "print_closest_cosine(glove[\"cat\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse 0.0\n",
      "doctor 3.1274529\n",
      "dentist 3.1306615\n",
      "nurses 3.26872\n",
      "pediatrician 3.3212206\n",
      "counselor 3.3987114\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['nurse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(glove['nurse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 0.0\n",
      "computers 2.4362664\n",
      "software 2.926823\n",
      "technology 3.1903508\n",
      "electronic 3.5067408\n",
      "computing 3.5999787\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(glove['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white 0.0\n",
      "black 2.294861\n",
      "green 2.597257\n",
      "gray 2.7076583\n",
      "brown 2.7215066\n",
      "blue 3.1592987\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['white'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(glove['white'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off-white 0.0\n",
      "yellowish-brown 2.1355708\n",
      "orange-brown 2.3458037\n",
      "yellow-brown 2.422701\n",
      "red-brown 2.4578924\n",
      "reddish-orange 2.6069083\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['off-white'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(glove['off-white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at which words are closest to the midpoints of two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicago 2.8645856\n",
      "tokyo 2.894505\n",
      "seattle 2.8945053\n",
      "york 3.0259786\n",
      "toronto 3.0561922\n",
      "phoenix 3.175262\n"
     ]
    }
   ],
   "source": [
    "print_closest_words((glove['seattle'] + glove['tokyo']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine((glove['seattle'] + glove['tokyo']) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "One surprising aspect of GloVe vectors is that the *directions* in the\n",
    "embedding space can be meaningful. The structure of the GloVe vectors\n",
    "certain analogy-like relationship like this tend to hold:\n",
    "\n",
    "$$ king - man + woman \\approx queen $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return v / v.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['king']) + normalize(glove['woman'] - glove['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get reasonable answers like \"queen\"\n",
    "\n",
    "We can likewise flip the analogy around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['queen']) + normalize(glove['man'] - glove['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, try a different but related analogies along the gender axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['king']) + normalize(glove['princess'] - glove['prince']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['grandfather']) + normalize(glove['woman'] - glove['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: make a capital city predictor\n",
    "\n",
    "Use the analogy idea to try to make a capital city predictor, so for example, when you ask\n",
    "\n",
    "`guess_capital('japan')` it says `tokyo` or `guess_capital('france')` it says `paris`.\n",
    "\n",
    "Can you improve the accuracy of the predictor by averaging a few cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of guess_capital needed\n",
      "None\n",
      "Your implementation of guess_capital needed\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def guess_capital(country):\n",
    "    print('Your implementation of guess_capital needed')\n",
    "\n",
    "print(guess_capital('japan'))\n",
    "print(guess_capital('france'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases in Word Vectors\n",
    "\n",
    "Machine learning models have an air of \"fairness\" about them, since models\n",
    "make decisions without human intervention. However, models can and do learn\n",
    "whatever bias is present in the training data!\n",
    "\n",
    "GloVe vectors seems innocuous enough: they are just representations of\n",
    "words in some embedding space. Even so, we'll show that the structure\n",
    "of the GloVe vectors encodes the everyday biases present in the texts\n",
    "that they are trained on.\n",
    "\n",
    "We'll start with an example analogy:\n",
    "\n",
    "$$ doctor - man + woman \\approx ?? $$\n",
    "\n",
    "Let's use GloVe vectors to find the answer to the above analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['doctor']) + normalize(glove['woman'] - glove['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $$doctor - man + woman \\approx nurse$$ analogy is very concerning.\n",
    "Just to verify, the same result does not appear if we flip the gender terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['doctor']) + normalize(glove['man'] - glove['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see similar types of gender bias with other professions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['banker']) + normalize(glove['woman'] - glove['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, if we flip the gender terms, we get very\n",
    "different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation of print_closest_cosine needed\n"
     ]
    }
   ],
   "source": [
    "print_closest_cosine(normalize(glove['banker']) + normalize(glove['man'] - glove['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results for \"engineer\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: find other biases\n",
    "\n",
    "Can you find other biases?\n",
    "\n",
    "For example, what does GloVe say about a (woman-man) company founder?\n",
    "\n",
    "What about a (man-woman) parent or teacher or housekeeper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore the structure of GloVe embedding space here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
